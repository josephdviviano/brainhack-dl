%&pdflatex
%% filename: amsart-template.tex, version: 2.1
\documentclass[reqno]{amsart}

\usepackage{hyperref}
\usepackage{inputenc}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{float}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\numberwithin{equation}{section}
\setlength{\parindent}{0pt} % turn off auto-indent

\begin{document}

\title{Building a MLP the Hard Way}

\author{Joseph D. Viviano}
\address{Universit\'e de Montr\'eal}
\curraddr{}
\email{joseph@viviano.ca}
\thanks{}
\date{Jul 2019}

\maketitle

\section{Standardize input data}

See \texttt{make\_mnist\_proc}. Image data is typically scaled so that each
channel has zero mean. \\

\textbf{IMPORTANT:} the mean of each channel should be calculated on the
TRAINING data only and applied to the VALID and TEST data. Otherwise you have
information leakage between your TRAIN, VALID, and TEST sets which can give
you higher (but invalid) results!

\section{Weight initialization}

See \texttt{\_init\_W}. Neural network weights should be initialized to have small
random normally distributed values. Actually, people do even fancier things
(i.e., Xavier initialization) but for this task, it is fine to just draw the
weights from a gaussian. \\

Later on: try setting all the weights to zero and see what happens when you
train the model!

\section{Forward propagation}

See \texttt{fprop}. Fill in the 4 calculations for the forward pass. \\

Below are the equations!

The dimension of $\mathbf{b}^{(1)}$ is: $\mathbf{b^{(1)}} \in \mathbb{R}^{d_h}$. \\

\begin{equation}
    \mathbf{h}^a = \mathbf{W}^{(1)T} \cdot \mathbf{x} + \mathbf{b}^{(1)}
\end{equation}

\begin{equation}
    \mathbf{h}^s = g(\mathbf{h}^a)
\end{equation}

Where $g(x)$ is the activation function (i.e., ReLU nonlinearity) applied
element wise to the hidden layer. $g(x) = max(0,x)$. \\

The dimensions of $\mathbf{W}^{(2)}$ and $\mathbf{b}^{(2)}$ are: $\mathbf{W}^{(2)} \in \mathbb{R}^{m \times d_h}$, and $\mathbf{b}^{(2)} \in \mathbb{R}^{m}$. \\

\begin{equation}
    \mathbf{o}^a = \mathbf{W}^{(2)T} \cdot \mathbf{h^s}+ \mathbf{b}^{(2)}
\end{equation}

\begin{equation}
    \mathbf{o}_k^a = \mathbf{W}_k^{(2)T} \cdot \mathbf{h^s}+ b_k^{(2)}
\end{equation}

Let's define an m-class softmax: $softmax(x) = \frac{e^{x_i}}{\sum_{i=1}^m e^{x_i}}$. Therefore, \\

\begin{equation}
    \mathbf{o}_k^s = \frac{e^{\mathbf{o}_k^a}}{\sum_{i=1}^m e^{\mathbf{o}_k^a}}
\end{equation}

\begin{align}
\sum_{i=1}^m \mathbf{o}_k^s
&= \sum_{i=1}^m \frac{e^{\mathbf{o}_k^a}}{\sum_{i=1}^m e^{\mathbf{o}_k^a}} \\
&= \frac{\sum_{i=1}^m e^{\mathbf{o}_k^a}}{\sum_{i=1}^m e^{\mathbf{o}_k^a}} \\
&= 1.
\end{align}

Since $e^x > 0$, the result will always be positive. This is crucial because
we need the softmax to produce a probability  distribution over our $m$ output
classes. \\

\section{Backpropagation (with clues!)}

See \texttt{bprop}. Fill in the 8 calculations for the backward pass. Fill
in the remaining 4 equations to do the parameter updates. \\

\subsection{Optimization}

The empirical risk $\hat{R}$ associated with the loss function (with a
regulatrization term) is:

\begin{equation}
    \hat{R} = \frac{1}{n}\sum_{i=1}^{n} L(f_\theta(\mathbf{x}^{(i)}),y) + \lambda\Omega(\theta)
\end{equation}\\

We proceed via gradient descent, i.e., we are going to optimize the parameters
of the network such that we minimize this risk generated by some function
$f_\theta$ shown some dataset $D_n$.

\begin{equation}
    \theta^* = argmin \hat{R}_\lambda(f_\theta, D_n)
\end{equation}

The gradient descent update looks like this:

\begin{align}
    \theta &\leftarrow \theta - \eta \frac{ \partial \hat{R} \lambda}{ \partial \theta} + \eta \frac{\partial}{\partial\theta}\Omega(\theta)
    &\leftarrow \theta - \eta \frac{\partial}{\partial\theta}
    \Bigg(\frac{1}{n}\sum_{i=1}^n  L(f_\theta(\mathbf{x}^{(i)}), y^{(1)}) \Bigg) + \eta \frac{\partial}{\partial\theta}\Omega(\theta)
\end{align}

\subsection{Loss of the prediction and it's derivative}

Now that we have done a forward pass to generate our predictions, we need to
calculate the loss so we can figure out how to update the weights. We define
the loss function (you don't need to implement this, but you need to call it
appropriately). It is here for explaination, and in the code, as well. \\

\begin{equation}
L(\mathbf{o}^a, y) = -\sum_{k=1}^M y_{k}\log \Bigg(
    \frac{e^{\mathbf{o}_k^a}}{\sum_{i=1}^m e^{\mathbf{o}_k^a}}
\Bigg)
\end{equation}

Now we are ready to take the derivative with respect to the output layer when
we have the correct class k (vs. the incorrect class i): \\

\begin{align}
    \frac{\partial L}{\partial \mathbf{o}_k^a} &=
        -1 + \frac{e^{\mathbf{o}_k^a}}{\sum_{i \neq k}^m e^{\mathbf{o}_i^a} + e^{\mathbf{o}_k^a}} \\
    &= \frac{e^{\mathbf{o}_k^a}}{\sum_{k=1}^m e^{\mathbf{o}_k^a}} -1 \\
    &= \mathbf{o}^s_k - 1
\end{align} \\

Since $onehot_m(y) = 1$ when $m$ is the target and is $0$ otherwise, we see that the above is true. We can similarly take the derivative with respect to the output layer when we have the incorrect class i: \\

\begin{align}
    \frac{\partial L}{\partial \mathbf{o}_i^a} &=
        \frac{e^{\mathbf{o}_i^a}}{\sum_{i \neq k}^m e^{\mathbf{o}_i^a} + e^{\mathbf{o}_k^a}} \\
    &= \mathbf{o}^s_i - 0
\end{align}

See \texttt{softmax\_backward} and \texttt{\_softmax} for an implementation.

\subsection{Backward update to the second weight matrix and bias term}

Note that all \textbf{bold} variables represent matrices. \\

We have already defined $\frac{\partial L}{\partial o^a_k}$. $\frac{\partial o_k^a}{\partial W_{kj}^{(2)}}$ and $\frac{\partial \mathbf{o}_k^a}{\partial b_k^{(2)}}$ are given by:\\

\begin{equation}
    \frac{\partial L}{\partial \mathbf{W}^{(2)}} = \frac{\partial L}{\partial \mathbf{o}^a}\frac{\partial \mathbf{o}^a}{\partial \mathbf{W}^{(2)}}
\end{equation}

\begin{align}
    \frac{\partial \mathbf{o}^a}{\partial \mathbf{W}^{(2)}} &= \frac{\partial}{\partial \mathbf{W}^{(2)}} (\mathbf{W}^{(2)T} \mathbf{h}^s + \mathbf{b}^{(2)}) \\
    &= \mathbf{h}^{sT}
\end{align}

and

\begin{equation}
    \frac{\partial L}{\partial \mathbf{b}^{(2)}} = \frac{\partial L}{\partial \mathbf{o}^a}\frac{\partial \mathbf{o}^a}{\partial \mathbf{b}^{(2)}}
\end{equation}

\begin{align}
    \frac{\partial \mathbf{o}^a}{\partial \mathbf{b}^{(2)}} &= \frac{\partial}{\partial \mathbf{b}^{(2)}} (\mathbf{W}^{(2)T} \mathbf{h}^s + \mathbf{b}^{(2)}) \\
    &= \mathbf{1}
\end{align}


\subsection{Backward through the weights}

\begin{equation}
    \frac{\partial L}{\partial \mathbf{h}^s} = \frac{\partial L}{\partial \mathbf{o}^a} \frac{\partial \mathbf{o}^a}{\partial \mathbf{h}^s}
\end{equation}\\

We have aleady defined $\frac{\partial L}{\partial \mathbf{o}^a}$. The gradient of $\frac{\partial \mathbf{o}^a}{\partial \mathbf{h}^s}$ is given by:\\

\begin{align}
    \frac{\partial \mathbf{o}^a}{\partial \mathbf{h}^s} &= \frac{\partial}{\partial \mathbf{h}^s} (\mathbf{W}^{(2)T}\mathbf{h}^s +\mathbf{b}^{(2)})\\
    &= \mathbf{W}^{(2)T}
\end{align}

\subsection{Backward through the ReLU}

\begin{equation}
    \frac{\partial L}{\partial \mathbf{h}^a} = \frac{\partial L}{\partial \mathbf{h}^s} \frac{\partial \mathbf{h}^s}{\partial \mathbf{h}}
\end{equation}

Where:

\begin{equation}
    \frac{\partial \mathbf{h}^s}{\partial \mathbf{h}^a} = \mathbf{I}_{\{h_j^a > 0\}}
\end{equation}

Where $\mathbf{I} \in \mathbb{R}^{d_h}$.\\

\subsection{Backward update to the first weights and biases}

We have already defined $\frac{\partial L}{\partial h^a_j}$. \\

\begin{equation}
    \frac{\partial L}{\partial \mathbf{W}^{(1)}} = \frac{\partial L}{\partial \mathbf{h}^a}\frac{\partial \mathbf{h}^a}{ \mathbf{W}^{(1)}}
\end{equation}

\begin{align}
    \frac{\partial \mathbf{h}^a}{ \mathbf{W}^{(1)}} &= \frac{\partial}{\mathbf{W}^{(1)}} (\mathbf{W}^{(1)} \mathbf{x}^T+ \mathbf{b}^{(1)}) \\
    &= \mathbf{x}^T
\end{align}

and

\begin{equation}
    \frac{\partial L}{\partial \mathbf{b}^{(1)}} = \frac{\partial L}{\partial \mathbf{h}^a}\frac{\partial \mathbf{h}^a}{ \mathbf{b}^{(1)}}
\end{equation}

\begin{align}
    \frac{\partial \mathbf{h}^a}{ \partial \mathbf{b}^{(1)}} &= \frac{\partial}{\mathbf{b}^{(1)}} (\mathbf{W}^{(1)} \mathbf{x}^T+ \mathbf{b}^{(1)}) \\
    &= \mathbf{1}
\end{align}

Where $\mathbf{h}^{a} \in \mathbb{R}^{d_h}$, $\mathbf{W}^{(1)} \in \mathbb{R}^{d_h \times d}$, $\mathbf{b}^{(1)} \in \mathbb{R}^{d_h}$, $\mathbf{x} \in \mathbb{R}^{d}$ and $\mathbf{1} \in \mathbb{R}^{d_h}$\\

\subsection{Backward update through the first weights}

\begin{equation}
    \frac{\partial L}{\partial \mathbf{x}} = \frac{\partial L}{\partial \mathbf{h}^a}\frac{\partial \mathbf{h}^a}{ \mathbf{x}}
\end{equation}\\

We have already defined $\frac{\partial L}{\partial \mathbf{h}^a}$. The gradient $\frac{\partial \mathbf{h}^a}{ \mathbf{x}}$ is given by: \\

\begin{align}
    \frac{\partial \mathbf{h}^a}{ \mathbf{x}} &= \frac{\partial}{\mathbf{x}} (\mathbf{W}^{(1)} \mathbf{x}^T+ \mathbf{b}^{(1)}) \\
    &= \mathbf{W}^{(1)T}
\end{align}

\subsection{Gradient of Regularizers}

You don't have to implement this, but we're showing it to you here. \\

What you see below is called \textit{elastic net} regularization. We have an
l1 and l2 penalty on the weights of the model:

\begin{align}
    L(\theta) &= L(\mathbf{W}^{(1)}, \mathbf{b}^{(1)}, \mathbf{W}^{(2)}, \mathbf{b}^{(2)}) \\
    &= \lambda_{11} ||\mathbf{W}^{(1)}||_1 + \lambda_{12} ||\mathbf{W}^{(1)}||_2^2 + \lambda_{21} ||\mathbf{W}^{(2)}||_1 + \lambda_{21} ||\mathbf{W}^{(2)}||_2^2
\end{align}\\

The gradient of the regularizers are added to the unregularized gradient of the
parameters to update. The regularizers affect only the parameters
$\mathbf{W}^{(1)}$ and $\mathbf{W}^{(2)}$. \\

\begin{equation}
    \Delta_{W^{(1)}} = - \frac{\partial L}{\partial \mathbf{W}^{(1)}} - \lambda_{11}sign(w^{(1)}_j) - \lambda_{12}[2\mathbf{W}^{(1)}]
\end{equation}

\begin{equation}
    \Delta_{W^{(2)}} = - \frac{\partial L}{\partial \mathbf{W}^{(2)}} - \lambda_{21}sign(w^{(2)}_j) - \lambda_{22}[2\mathbf{W}^{(2)}]
\end{equation}

\section{Training loop}

See \texttt{train}. \\

Split the data into training, testing, and validation data appropriately. Sample
minibatches of data to train the model using a forward pass and backward pass. \\

This should give you a good idea of how a typical "training loop" should be
structured.

\section{Batch vs minibatch training \& Gradient Checking}

See \texttt{exp1}. \\

Here, we can explore the differences between training with stochastic gradient
descent, minibatch stochastic gradient descent, and batch gradient descent.
You can use \texttt{time.time\(\)} to see how this decision effects execution
time. \\

This function will also do gradient checking (optional:
see \texttt{grad\_check}).

\section{Network capacity}

See \texttt{exp2}. \\

The hyperparamters you choose for your network (number of nodes in the hidden
layer, regularization terms, learning rate, etc.) all effect the capactiy
of your network. \\

Here you can play with the \texttt{options} dictionary and visualize the effect
on the decision boundary for the two circles dataset. If you want to make your
life easy, you can make a list of options and loop through them all. Inspect the
outputs to understand how different decisions affect your model.

\section{Experiment tracking}

See \texttt{exp3}. \\

It is important to learn how to monitor a deep learning experiment. Remember,
we want to build models that generalize well. Your task here is to train a model
using good hyperparameters and plot the results obtained from training the
model for the train, valid, and test sets. It is instructive to look at both
the accuracy of the model, and the loss obtained, over epochs. These are called
"training curves" and are common in deep learning research.

\end{document}
